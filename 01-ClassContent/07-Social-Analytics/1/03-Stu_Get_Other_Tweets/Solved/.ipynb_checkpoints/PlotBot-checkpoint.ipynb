{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tweepy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime #To get the current date\n",
    "from os import path, makedirs, remove, rmdir # fetch path and makedirs function from os file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keys from the config file\n",
    "# from config import consumer_key, consumer_secret, access_token, access_token_secret\n",
    "\n",
    "# Twitter API Keys\n",
    "consumer_key = 'b2jDtC9vAdTpnuJDZkNPkwEYF'\n",
    "consumer_secret = 'xEo1L3eRSaE6J7uOIlYP0ohoNp0jikv3LKunjp9arE0YWn5tVo'\n",
    "access_token = '57237295-K2EPPUBgXZfz83NuA92RxkzETWThD4YW8lt3bhAcW'\n",
    "access_token_secret = '9GD3xt89mMZNCKWmDwQvnaWw3tQjaz0vgz4l4d90Fb7N0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the directory if it does not exist\n",
    "filePath = 'PyBotOutput'\n",
    "if not path.exists(filePath):\n",
    "    makedirs(filePath)\n",
    "    \n",
    "    \n",
    "filePath = 'PyBotImages'\n",
    "if not path.exists(filePath):\n",
    "    makedirs(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment(sentiment_df,user):\n",
    "    sentiment_df.plot(\"Tweets Ago\",\"Compound\",marker=\"o\"\n",
    "                      , linewidth=0.8, alpha=0.8,linestyle=\"-\",label=user)\n",
    "    plt.legend(bbox_to_anchor = (1,1),title=\"Tweets\")\n",
    "    \n",
    "    plt.xlabel(\"Tweets Ago\")\n",
    "    plt.ylabel(\"Tweet Polarity\")\n",
    "    plt.title(\"Sentiment Analysis of %s (%s)\"%(user.replace(\"@\",\"\")\n",
    "                                               ,datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    \n",
    "    plt.xlim(502,-2)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    \n",
    "    # filename = \"Sentiment_Analysis_of_\"+user.replace(\"@\",\"\")+\".png\"\n",
    "    filename_png = \"PyBotImages/Sentiment_Analysis_of_\" + user.replace(\"@\",\"\") + \".png\"\n",
    "    plt.savefig(filename_png,bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    return filename_png\n",
    "\n",
    "\n",
    "def Retrieve_Tweets(user):\n",
    "    # Array to hold sentiment\n",
    "    Sentiment_array = []\n",
    "    # Setting the tweet count as 500\n",
    "    tweetcount = 500\n",
    "    print(\"Extracting tweets from %s\"%user)\n",
    "\n",
    "    # Extracting  pages of tweets\n",
    "    for x in range(25):\n",
    "        public_tweets = api.user_timeline(user,page = x)\n",
    "        # For each tweet \n",
    "        for tweet in public_tweets:\n",
    "            #Calculating the compound,positive,negative and neutral value for each tweet\n",
    "            compound = analyzer.polarity_scores(tweet[\"text\"])[\"compound\"]\n",
    "            pos = analyzer.polarity_scores(tweet[\"text\"])[\"pos\"]\n",
    "            neu = analyzer.polarity_scores(tweet[\"text\"])[\"neu\"]\n",
    "            neg = analyzer.polarity_scores(tweet[\"text\"])[\"neg\"]\n",
    "            # Store Tweet in Array\n",
    "            Sentiment_array.append({\"Media\":user,\n",
    "                                    \"Tweet Text\":tweet[\"text\"],\n",
    "                                    \"Compound\":compound,\n",
    "                                    \"Positive\":pos,\n",
    "                                    \"Negative\":neg,\n",
    "                                    \"Neutral\":neu,\n",
    "                                    \"Date\":tweet[\"created_at\"],\n",
    "                                    \"Tweets Ago\":tweetcount})\n",
    "            #Decreasing tweet count by 1\n",
    "            tweetcount-=1\n",
    "\n",
    "    print(\"End of Extraction of Tweets\")\n",
    "\n",
    "    # Creating a dataframe from the Sentiment Array\n",
    "    Sentiment_DF=pd.DataFrame.from_dict(Sentiment_array)\n",
    "    \n",
    "    # Removing the '@' from Media column in the data frame\n",
    "    Sentiment_DF['Media'] = Sentiment_DF['Media'].map(lambda x: x.lstrip('@'))\n",
    "\n",
    "    # Re_arranging the order of columns before saving into CSV file\n",
    "    Sentiment_DF = Sentiment_DF[[\"Media\",\"Date\",\"Tweet Text\",\"Compound\"\n",
    "                                 ,\"Positive\",\"Negative\",\"Neutral\",\"Tweets Ago\"]]\n",
    "    \n",
    "    Sentiment_DF.to_csv(\"PyBotOutput/Sentiment_Analysis_of_\" + user.replace(\"@\",\"\")+\".csv\")\n",
    "    \n",
    "    # filename_csv = \"PyBotOutput/Sentiment_Analysis_of_\"+user.replace(\"@\",\"\")+\".csv\"\n",
    "    # plt.savefig(filename_csv,bbox_inches='tight')\n",
    "    # filename_csv = plot_sentiment(Sentiment_DF,user)\n",
    "    #return filename_csv\n",
    "#filename_csv\n",
    "\n",
    "\n",
    "\n",
    "#     plt.show()\n",
    "#     return filename_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # filename = \"Sentiment_Analysis_of_\"+user.replace(\"@\",\"\")+\".png\"\n",
    "#     filename_png = \"PyBotImages/Sentiment_Analysis_of_\"+user.replace(\"@\",\"\")+\".png\"\n",
    "#     plt.savefig(filename_png,bbox_inches='tight')\n",
    "#     plt.show()\n",
    "#     return filename_png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Query_DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-90e017979e6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;31m# Append the array into the main dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mQuery_DF\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQuery_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m \u001b[0mQuery_DF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;31m#             # If there is no request from user mentions then the Tweet request format is incorrect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;31m#             else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Query_DF' is not defined"
     ]
    }
   ],
   "source": [
    "def Search_Request():\n",
    "    # An array to store the neccessary information on each tweet\n",
    "    Query_array = []\n",
    "    \n",
    "    # If the file Request.csv exists \n",
    "    try:\n",
    "        # Read the csv file\n",
    "        Query_DF = pd.read_csv(\"PyBotOutput/Requests.csv\")\n",
    "        # Get the last id from the first row of the csv file\n",
    "        lastid = Query_DF[\"Tweet ID\"][Query_DF.index[0]]\n",
    "        # Get the request using api.search\n",
    "        request = api.search(\"#InaPyBot\",since_id = lastid)\n",
    "        \n",
    "        \n",
    "        # For each new tweet \n",
    "        for tweet in request[\"statuses\"]:\n",
    "            \n",
    "            # Check if there is requests for user mention from the tweet\n",
    "            if not(not tweet[\"entities\"][\"user_mentions\"]):\n",
    "                \n",
    "                # Check if there is any previous request for that user mention with the content of csv file\n",
    "                if Query_DF[\"User Request\"].all() != tweet[\"entities\"][\"user_mentions\"][0][\"screen_name\"]:\n",
    "                    # Appending the necessary details to the array\n",
    "                    Query_array.append({\"User Name\":tweet[\"user\"][\"screen_name\"],\n",
    "                                \"User Request\":tweet[\"entities\"][\"user_mentions\"][0][\"screen_name\"],\n",
    "                                \"Tweet ID\":tweet[\"id\"],\n",
    "                                \"Created On\":tweet[\"created_at\"]\n",
    "                                })\n",
    "                    # Retrieve and plot the graph for the requested user mention\n",
    "                    filename = Retrieve_Tweets(tweet[\"entities\"][\"user_mentions\"][0][\"screen_name\"])\n",
    "                    reply_text = \"New Tweet Analysis: @%s (Thanks @%s)\"%(\n",
    "                            tweet[\"entities\"][\"user_mentions\"][0][\"screen_name\"]\n",
    "                           ,tweet[\"user\"][\"screen_name\"])\n",
    "                    # Reply to tweet with the image file\n",
    "                    api.update_with_media(filename=filename,status=reply_text,in_reply_to_status_id=tweet[\"id\"])\n",
    "                    \n",
    "                    # Convert the array into a dataframe\n",
    "                    New_DF = pd.DataFrame.from_dict(Query_array)\n",
    "                    # Append the new temporary array into the main dataframe\n",
    "                    Query_DF = New_DF.append(Query_DF)\n",
    "                    \n",
    "                # If a previous request for that user mention has already been processed\n",
    "                else:\n",
    "                    print(\"The request for %s has already been processed\"%tweet[\"entities\"][\"user_mentions\"][0][\"screen_name\"])\n",
    "                    # Try and error in case that status has already been processed and to avoid duplication error\n",
    "                    try:\n",
    "                        reply_text = \"Thank you for your tweet @%s! Sorry, %s request has already been tweeted!\"%(\n",
    "                                tweet[\"user\"][\"screen_name\"],\n",
    "                                tweet[\"entities\"][\"user_mentions\"][0][\"screen_name\"])\n",
    "                        # Reply to tweet with the Sorry Message\n",
    "                        api.update_status(reply_text,in_reply_to_status_id=tweet[\"id\"])\n",
    "                    except:\n",
    "                        pass\n",
    "    \n",
    "            # If there is no request from user mentions then the Tweet request format is incorrect\n",
    "            else:\n",
    "                reply_text = \"@%s \\nTweet text:%s \\nError: Not in the specified format or doesnot have an existing user in Twitter\"%(\n",
    "                    tweet[\"user\"][\"screen_name\"],tweet[\"text\"])\n",
    "                # Reply to tweet saying the format is incorrect\n",
    "                api.update_status(reply_text,in_reply_to_status_id = tweet[\"id\"])\n",
    "                print(reply_text)\n",
    "                \n",
    "        \n",
    "    # If the Request.csv file doesnot exist, this is mainly for the first time when the code is deployed\n",
    "    except FileNotFoundError:\n",
    "        \n",
    "        # Placing the first tweet id of one from my timeline\n",
    "        lastid = 1010923009731956739\n",
    "        # Searching for tweets using the handle and since the last id\n",
    "        request = api.search(\"#InaPyBot\",since_id=lastid)\n",
    "        \n",
    "        # For each tweet in the response\n",
    "        for tweet in request[\"statuses\"]:\n",
    "            \n",
    "            # Check if there is requests for user mention from the tweet\n",
    "            if not(not tweet[\"entities\"][\"user_mentions\"]):\n",
    "                # Appending the necessary details to the array\n",
    "                Query_array.append({\"User Name\":tweet[\"user\"][\"screen_name\"],\n",
    "                                \"User Request\":tweet[\"entities\"][\"user_mentions\"][0][\"screen_name\"],\n",
    "                                \"Tweet ID\":tweet[\"id\"],\n",
    "                                \"Created On\":tweet[\"created_at\"]\n",
    "                                })\n",
    "                # Retrieve and plot the graph for the requested user mention\n",
    "                filename = Retrieve_Tweets(tweet[\"entities\"][\"user_mentions\"][0][\"screen_name\"])\n",
    "                reply_text = \"New Tweet Analysis: @%s (Thanks @%s)\"%(\n",
    "                        tweet[\"entities\"][\"user_mentions\"][0][\"screen_name\"],\n",
    "                        tweet[\"user\"][\"screen_name\"])\n",
    "                \n",
    "                # Reply to tweet with the image file\n",
    "                api.update_with_media(filename=filename,status=reply_text,in_reply_to_status_id=tweet[\"id\"])\n",
    "                # Append the array into the main dataframe\n",
    "                Query_DF=pd.DataFrame.from_dict(Query_array)\n",
    "Query_DF.keys()           \n",
    "#             # If there is no request from user mentions then the Tweet request format is incorrect\n",
    "#             else:\n",
    "#                 reply_text = \"@%s \\nTweet text:%s \\nError: Not in the specified format or doesnot have an existing user in Twitter\"%(\n",
    "#                     tweet[\"user\"][\"screen_name\"],tweet[\"text\"])\n",
    "#                 # Reply to tweet saying the format is incorrect\n",
    "#                 api.update_status(reply_text,in_reply_to_status_id=tweet[\"id\"])\n",
    "#                 print(reply_text)\n",
    "\n",
    "                \n",
    "        #Query_DF = pd.DataFrame.from_dict(Query_array)\n",
    "    # Query_DF.to_csv(\"Requests.csv\",index = False)\n",
    "\n",
    "    #Query_DF.to_csv(\"PyBotOutput/Requests.csv\",index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'Query_DF' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-1ba95b0a4cec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mSearch_Request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-5d444f88f60e>\u001b[0m in \u001b[0;36mSearch_Request\u001b[1;34m()\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m#Query_DF = pd.DataFrame.from_dict(Query_array)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mQuery_DF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Requests.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;31m#Query_DF.to_csv(\"PyBotOutput/Requests.csv\",index = False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'Query_DF' referenced before assignment"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    Search_Request()\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
